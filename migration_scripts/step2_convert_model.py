# step2_convert_model.py
# PASO 2: Convertir modelo TensorFlow a ONNX

import tensorflow as tf
import tf2onnx
import onnxruntime as ort
import numpy as np
import json
from pathlib import Path
import time

def convert_tensorflow_to_onnx():
    """
    Convierte el modelo TensorFlow existente a formato ONNX
    """
    print("ğŸ”„ PASO 2: CONVERSIÃ“N TENSORFLOW â†’ ONNX")
    print("=" * 60)
    
    # Configurar rutas de archivos
    tf_model_path = Path("model/plant_classifier.h5")
    onnx_model_path = Path("model/plant_classifier.onnx")
    species_path = Path("model/species_list.json")
    
    # Verificar que los archivos originales existen
    print("ğŸ“‚ VERIFICANDO ARCHIVOS ORIGINALES...")
    
    if not tf_model_path.exists():
        print(f"âŒ No se encontrÃ³ el modelo TensorFlow: {tf_model_path}")
        print("ğŸ’¡ AsegÃºrate de que el archivo plant_classifier.h5 estÃ© en la carpeta model/")
        return False
    
    if not species_path.exists():
        print(f"âŒ No se encontrÃ³ la lista de especies: {species_path}")
        print("ğŸ’¡ AsegÃºrate de que el archivo species_list.json estÃ© en la carpeta model/")
        return False
    
    print(f"âœ… Modelo TensorFlow encontrado: {tf_model_path}")
    print(f"âœ… Lista de especies encontrada: {species_path}")
    
    # Paso 1: Cargar modelo TensorFlow
    print("\nğŸ”„ CARGANDO MODELO TENSORFLOW...")
    try:
        # Suprimir logs verbosos de TensorFlow
        tf.get_logger().setLevel('ERROR')
        
        model = tf.keras.models.load_model(str(tf_model_path))
        
        print(f"âœ… Modelo TensorFlow cargado exitosamente")
        print(f"   ğŸ“Š Input shape: {model.input_shape}")
        print(f"   ğŸ“Š Output shape: {model.output_shape}")
        print(f"   ğŸ“Š ParÃ¡metros totales: {model.count_params():,}")
        
        # Obtener informaciÃ³n del modelo
        input_shape = model.input_shape
        output_shape = model.output_shape
        
    except Exception as e:
        print(f"âŒ Error cargando modelo TensorFlow: {e}")
        return False
    
    # Paso 2: Cargar lista de especies
    print("\nğŸ“‹ CARGANDO LISTA DE ESPECIES...")
    try:
        with open(species_path, 'r', encoding='utf-8') as f:
            species_list = json.load(f)
        
        print(f"âœ… Lista de especies cargada: {len(species_list)} especies")
        print(f"   ğŸŒ¿ Primeras 3: {species_list[:3]}")
        
        # Verificar consistencia
        expected_classes = output_shape[-1]
        actual_species = len(species_list)
        
        if expected_classes != actual_species:
            print(f"âš ï¸ WARNING: Inconsistencia detectada:")
            print(f"   Modelo espera: {expected_classes} clases")
            print(f"   Lista tiene: {actual_species} especies")
            
            # Continuar pero advertir
            print("   Continuando con la conversiÃ³n...")
        
    except Exception as e:
        print(f"âŒ Error cargando lista de especies: {e}")
        return False
    
    # Paso 3: Preparar para conversiÃ³n
    print("\nğŸ”§ PREPARANDO CONVERSIÃ“N...")
    
    # Definir especificaciÃ³n de entrada (batch variable)
    input_signature = [tf.TensorSpec(
        shape=(None, 224, 224, 3), 
        dtype=tf.float32, 
        name="input_image"
    )]
    
    print(f"âœ… EspecificaciÃ³n de entrada preparada: {input_signature[0].shape}")
    
    # Paso 4: Convertir a ONNX
    print("\nğŸš€ INICIANDO CONVERSIÃ“N A ONNX...")
    print("   â±ï¸ Esto puede tardar 1-2 minutos...")
    
    start_time = time.time()
    
    try:
        # Realizar conversiÃ³n
        onnx_model, _ = tf2onnx.convert.from_keras(
            model,
            input_signature=input_signature,
            opset=13,  # VersiÃ³n ONNX estable y compatible
            output_path=str(onnx_model_path)
        )
        
        conversion_time = time.time() - start_time
        
        print(f"âœ… ConversiÃ³n completada exitosamente!")
        print(f"   â±ï¸ Tiempo de conversiÃ³n: {conversion_time:.2f} segundos")
        print(f"   ğŸ’¾ Modelo ONNX guardado: {onnx_model_path}")
        
    except Exception as e:
        print(f"âŒ Error durante la conversiÃ³n: {e}")
        print("ğŸ’¡ Posibles soluciones:")
        print("   - Verifica que el modelo TensorFlow sea vÃ¡lido")
        print("   - AsegÃºrate de tener suficiente memoria disponible")
        print("   - Intenta reiniciar Python y ejecutar de nuevo")
        return False
    
    # Paso 5: Verificar archivos generados
    print("\nğŸ“ VERIFICANDO ARCHIVOS GENERADOS...")
    
    if not onnx_model_path.exists():
        print(f"âŒ El archivo ONNX no se generÃ³: {onnx_model_path}")
        return False
    
    # Comparar tamaÃ±os
    tf_size = tf_model_path.stat().st_size / (1024 * 1024)  # MB
    onnx_size = onnx_model_path.stat().st_size / (1024 * 1024)  # MB
    reduction = ((tf_size - onnx_size) / tf_size) * 100
    
    print(f"âœ… Archivo ONNX generado correctamente")
    print(f"ğŸ“Š COMPARACIÃ“N DE TAMAÃ‘OS:")
    print(f"   TensorFlow (.h5): {tf_size:.2f} MB")
    print(f"   ONNX (.onnx): {onnx_size:.2f} MB")
    print(f"   ReducciÃ³n: {reduction:.1f}%")
    
    # Paso 6: Probar modelo ONNX
    print("\nğŸ§ª PROBANDO MODELO ONNX...")
    
    try:
        # Cargar sesiÃ³n ONNX
        print("   ğŸ”„ Cargando modelo ONNX...")
        ort_session = ort.InferenceSession(str(onnx_model_path))
        
        # Obtener informaciÃ³n de entrada y salida
        input_details = ort_session.get_inputs()[0]
        output_details = ort_session.get_outputs()[0]
        
        print(f"   âœ… Modelo ONNX cargado correctamente")
        print(f"   ğŸ“Š Input: {input_details.name} {input_details.shape}")
        print(f"   ğŸ“Š Output: {output_details.name} {output_details.shape}")
        
        # Crear imagen de prueba
        print("   ğŸ”„ Creando imagen de prueba...")
        test_image = np.random.random((1, 224, 224, 3)).astype(np.float32)
        
        # Hacer predicciÃ³n con ONNX
        print("   ğŸ”„ Haciendo predicciÃ³n de prueba con ONNX...")
        start_time = time.time()
        
        onnx_predictions = ort_session.run(
            [output_details.name], 
            {input_details.name: test_image}
        )[0]
        
        onnx_inference_time = time.time() - start_time
        
        # Hacer predicciÃ³n con TensorFlow (para comparar)
        print("   ğŸ”„ Haciendo predicciÃ³n de prueba con TensorFlow...")
        start_time = time.time()
        
        tf_predictions = model.predict(test_image, verbose=0)
        
        tf_inference_time = time.time() - start_time
        
        # Comparar resultados
        print(f"\nğŸ“Š COMPARACIÃ“N DE PREDICCIONES:")
        print(f"   â±ï¸ Tiempo ONNX: {onnx_inference_time:.4f}s")
        print(f"   â±ï¸ Tiempo TensorFlow: {tf_inference_time:.4f}s")
        print(f"   ğŸš€ Speedup: {tf_inference_time/onnx_inference_time:.2f}x mÃ¡s rÃ¡pido")
        
        # Verificar que las predicciones son similares
        max_diff = np.max(np.abs(onnx_predictions - tf_predictions))
        print(f"   ğŸ“ Diferencia mÃ¡xima: {max_diff:.6f}")
        
        if max_diff < 1e-5:
            print(f"   âœ… Predicciones idÃ©nticas (diferencia < 1e-5)")
        elif max_diff < 1e-3:
            print(f"   âœ… Predicciones muy similares (diferencia < 1e-3)")
        else:
            print(f"   âš ï¸ Predicciones difieren significativamente")
        
        # Mostrar top predicciÃ³n
        onnx_top_idx = np.argmax(onnx_predictions[0])
        tf_top_idx = np.argmax(tf_predictions[0])
        
        print(f"\nğŸ¯ RESULTADOS DE PRUEBA:")
        print(f"   ONNX top predicciÃ³n: Clase {onnx_top_idx} ({species_list[onnx_top_idx]})")
        print(f"   TensorFlow top predicciÃ³n: Clase {tf_top_idx} ({species_list[tf_top_idx]})")
        
        if onnx_top_idx == tf_top_idx:
            print(f"   âœ… Ambos modelos predicen la misma clase")
        else:
            print(f"   âš ï¸ Los modelos predicen clases diferentes")
        
    except Exception as e:
        print(f"âŒ Error probando modelo ONNX: {e}")
        return False
    
    # Paso 7: Resumen final
    print(f"\nğŸ‰ CONVERSIÃ“N COMPLETADA EXITOSAMENTE!")
    print("=" * 60)
    print("ğŸ“‹ RESUMEN:")
    print(f"   âœ… Modelo original: {tf_model_path}")
    print(f"   âœ… Modelo convertido: {onnx_model_path}")
    print(f"   ğŸ“Š ReducciÃ³n de tamaÃ±o: {reduction:.1f}%")
    print(f"   ğŸš€ Mejora de velocidad: {tf_inference_time/onnx_inference_time:.2f}x")
    print(f"   ğŸŒ¿ Especies soportadas: {len(species_list)}")
    
    print(f"\nğŸ’¡ PRÃ“XIMOS PASOS:")
    print(f"   2. âœ… COMPLETADO: ConversiÃ³n del modelo")
    print(f"   3. â³ SIGUIENTE: Actualizar cÃ³digo de Streamlit")
    
    return True

def show_next_steps():
    """Muestra informaciÃ³n sobre los prÃ³ximos pasos"""
    print("\n" + "="*60)
    print("ğŸ¯ ESTADO ACTUAL:")
    print("="*60)
    print("1. âœ… COMPLETADO: InstalaciÃ³n de dependencias")
    print("2. âœ… COMPLETADO: ConversiÃ³n del modelo TensorFlow â†’ ONNX")
    print("3. â³ SIGUIENTE: Actualizar cÃ³digo de Streamlit para usar ONNX")
    print("4. â¸ï¸  PENDIENTE: Actualizar requirements.txt")
    print("5. â¸ï¸  PENDIENTE: Deploy a Streamlit Cloud")
    
    print("\nğŸ’¡ ARCHIVOS GENERADOS:")
    print("   - model/plant_classifier.onnx (nuevo modelo)")
    print("   - model/plant_classifier.h5 (modelo original, conservado)")
    print("   - model/species_list.json (sin cambios)")

if __name__ == "__main__":
    print("ğŸš€ MIGRACIÃ“N A ONNX RUNTIME - PASO 2")
    print("ConversiÃ³n del modelo TensorFlow â†’ ONNX")
    print()
    
    success = convert_tensorflow_to_onnx()
    
    if success:
        show_next_steps()
        print("\nğŸŸ¢ PASO 2 COMPLETADO - Confirma que todo funciona antes de continuar")
    else:
        print("\nğŸ”´ PASO 2 FALLÃ“ - Revisa los errores antes de continuar")
        print("\nğŸ’¡ Si hay problemas:")
        print("   - Verifica que model/plant_classifier.h5 existe")
        print("   - Verifica que model/species_list.json exists")
        print("   - AsegÃºrate de tener suficiente memoria (>4GB)")
        print("   - Reinicia Python y vuelve a intentar")